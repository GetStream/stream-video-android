---
title: Livestream Tutorial
description: How to build a livestream experience using Stream's video SDKs
---

import { TokenSnippet } from '../../../shared/_tokenSnippet.jsx';

## Livestream Tutorial

In this tutorial we'll quickly build a low-latency in-app livestreaming experience.
The livestream is broadcasted using Stream's edge network of servers around the world.
We'll cover the following topics:

* Ultra low latency streaming
* Multiple streams & co-hosts
* RTMP in and WebRTC input
* Exporting to HLS
* Reactions, custom events and chat
* Recording & Transcriptions

Let's get started, if you have any questions or feedback be sure to let us know via the feedback button.

### Step 1 - Create a new project in Android Studio

This tutorial was written using Android Studio Flamingo.
Setup steps can vary slightly across Android Studio versions so if you run into trouble be sure to use the latest version of Android Studio.

1. Create a new project
2. Select Phone & Template -> **empty activity**
3. Name your project **Livestream**.

### Step 2 - Install the SDK & Setup the client

**Add the video SDK** to your app's `build.gradle` file found in app/build.gradle.
If you're new to android note that there are 2 build.gradle files, you want to open the one in the app folder.

```groovy
dependencies {
    implementation "io.getstream:stream-video-android-compose:$stream_version"
}
```

This tutorial uses the compose version of the video SDK. Stream also provides a core library without compose.

### Step 3 - Broadcast a livestream from your phone

The following code shows how to publish from your phone's camera.
Let's open `MainActivity.kt` and replace the `MainActivity` class with the following code:

```kotlin
class MainActivity : ComponentActivity() {
    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)

        val userToken = "REPLACE_WITH_TOKEN"
        val userId = "REPLACE_WITH_USER_ID"
        val callId = "REPLACE_WITH_CALL_ID"

        // create a user.
        val user = User(
            id = userId, // any string
            name = "Tutorial" // name and image are used in the UI
        )

        // for a production app we recommend adding the client to your Application class or di module.
        val client = StreamVideoBuilder(
            context = applicationContext,
            apiKey = "mmhfdzb5evj2", // demo API key
            geo = GEO.GlobalEdgeNetwork,
            user = user,
            token = userToken,
        ).build()

        // join a call, which type is `default`
        val call = client.call("livestream", callId)
        lifecycleScope.launch {

            call.camera.enable()
            call.microphone.enable()

            // join the call
            val result = call.join(create = true)
            result.onError {
                Toast.makeText(applicationContext, "uh oh $it", Toast.LENGTH_SHORT).show()
            }
        }

        setContent {
            // request the Android runtime permissions for the camera and microphone
            LaunchCallPermissions(call = call)

            VideoTheme {
                Text("TODO: render video")
            }
        }
    }
}
```

You'll notice that these first 3 lines need their values replaced.

```kotlin
val userToken = "REPLACE_WITH_TOKEN"
val userId = "REPLACE_WITH_USER_ID"
val callId = "REPLACE_WITH_CALL_ID"
```

Replace them now with the values shown below:

<TokenSnippet sampleApp='livestream' displayStyle='credentials' />

In the next step we setup the user:

```kotlin
val user = User(
    id = userId, // any string
    name = "Tutorial" // name and image are used in the UI
)
```

If you don't have an authenticated user you can also use a guest or anonymous user.
For most apps it's convenient to match your own system of users to grant and remove permissions.

Next we create the client:

```kotlin
val client = StreamVideoBuilder(
    context = applicationContext,
    apiKey = "mmhfdzb5evj2", // demo API key
    geo = GEO.GlobalEdgeNetwork,
    user = user,
    token = userToken,
).build()
```

You'll see the `userToken` variable. Your backend typically generates the user token on signup or login.

The most important step to review is how we create the call.
Stream uses the same call object for livestreaming, audio rooms and video calling.
Have a look at the code snippet below:

```kotlin
val call = client.call("livestream", callId)
lifecycleScope.launch {

    call.camera.enable()
    call.microphone.enable()

    // join the call
    val result = call.join(create = true)
    result.onError {
        Toast.makeText(applicationContext, "uh oh $it", Toast.LENGTH_SHORT).show()
    }
}
```

First call object is created by specifying the call type: "livestream" and the callId.
The "livestream" call type is just a set a defaults that typically works well for a livestream.
You can edit the features, permissions and settings in the dashboard. You can also create new call types as needed.

Next call.camera.enable and microphone.enable start capturing the local audio and video.

Lastly, call.join(create = true) creates the call object on our servers and joins it.
The moment you use call.join() the realtime transport for audio and video is started.

Note that you can also add members to a call and assign them different roles. (See the [call creation docs](../03-guides/02-joining-creating-calls.mdx))

### Step 4 - Render the video

In this step we're going to build a UI for showing your local video with a button to start the livestream.
This example uses Compose, but you could also use our XML VideoRenderer.

In `MainActivity.kt` replace the `VideoTheme` with the following code.

```kotlin
VideoTheme {
    val participantCount by call.state.participantCounts.collectAsState()
    val total = participantCount?.total
    val backstage by call.state.backstage.collectAsState()
    val me by call.state.me.collectAsState()
    val video = me?.video?.collectAsState()
    val session by call.state.session.collectAsState()
    val durationInMs by call.state.durationInMs.collectAsState()

    Scaffold(
        modifier = Modifier
            .fillMaxSize()
            .background(VideoTheme.colors.appBackground)
            .padding(6.dp),
        contentColor = VideoTheme.colors.appBackground,
        backgroundColor = VideoTheme.colors.appBackground,
        topBar = {
            if (connection == RealtimeConnection.Connected) {
                if (!backstage) {
                    Box(
                        modifier = Modifier
                            .fillMaxWidth()
                            .padding(6.dp)
                    ) {
                        Text(
                            modifier = Modifier
                                .align(Alignment.CenterEnd)
                                .background(
                                    color = VideoTheme.colors.primaryAccent,
                                    shape = RoundedCornerShape(6.dp)
                                )
                                .padding(horizontal = 12.dp, vertical = 4.dp),
                            text = "Live $total",
                            color = VideoTheme.colors.textHighEmphasis
                        )

                        Text(
                            modifier = Modifier.align(Alignment.Center),
                            text = "Live for 1:23",
                            color = VideoTheme.colors.textHighEmphasis
                        )
                    }
                }
            }
        },
        bottomBar = {
            Button(
                colors = ButtonDefaults.buttonColors(
                    backgroundColor = VideoTheme.colors.primaryAccent,
                    contentColor = VideoTheme.colors.primaryAccent
                ),
                onClick = {
                    lifecycleScope.launch {
                        if (backstage) call.goLive() else call.stopLive()
                    }
                }
            ) {
                Text(
                    text = if (backstage) "Go Live" else "Stop Broadcast",
                    color = Color.White
                )
            }
        }
    ) {
        VideoRenderer(
            modifier = Modifier
                .fillMaxSize()
                .padding(it)
                .clip(RoundedCornerShape(6.dp)),
            call = call,
            video = video,
            videoFallbackContent = {
                Text(text = "Video rendering failed")
            }
        )
    }
}
```

If you now run your app you should see an interface like this:

![Livestream](../assets/tutorial-livestream.png)

Stream uses a technology called SFU cascading to replicate your livestream over different SFUs around the world.
This makes it possible to reach a large audience in realtime.

Now let's press "go live" in the android app and click the link below to watch the video in your browser.

<TokenSnippet sampleApp='livestream' displayStyle='join' />

Let's take a moment to review the Compose code above. Call.state exposes all the stateflow objects you need.

The most important ones are:

```
call.state
call.state.participants
```

The [participant state docs](../03-guides/03-call-and-participant-state.mdx) show all the available data.

The livestream layout is standard Compose other than [VideoRenderer](../04-ui-components/02-video-renderer.mdx).
VideoRenderer renders the video and a fallback. You can use it for rendering the local and remote video.

### Step 4 - (Optional) Publishing RTMP using OBS

The example above showed how to publish your phone's camera to the livestream.
Almost all livestream software and hardware supports RTMPs.
So let's see how to publish using RTMPs. Feel free to skip this step if you don't need to use RTMPs.

A. Log the URL & Stream Key

```kotlin
val rtmp = call.state.ingress.rtmp
Log.i("Tutorial", "RTMP url and streamingKey: $rtmp")
```

B. Open OBS and go to settings -> stream

- Select "custom" service
- Server: equal to the server URL from the log
- Stream key: equal to the stream key from the log

Press start streaming in OBS. The RTMP stream will now show up in your call just like a regular video participant.
Now that we've learned to publish using WebRTC or RTMP let's talk about watching the livestream.

### Step 5 - Viewing a livestream (WebRTC)

Watching a livestream is even easier than broadcasting.

Compared to the current code in in `MainActivity.kt` you:

* Don't need to request permissions or enable the camera
* Don't render the local video, but instead render the remote video
* Typically include some small UI elements like viewer count, a button to mute etc

The [docs on building a UI for watching a livestream](../05-ui-cookbook/15-watching-livestream.mdx)) explain this in more detail.

### Step 6 - (Optional) Viewing a livestream with HLS

Another way to watch a livestream is using HLS. HLS tends to have a 10 to 20 seconds delay, while the above WebRTC approach is realtime.
The benefit that HLS offers is better buffering under poor network conditions.
So HLS can be a good option when:

* A 10-20 second delay is acceptable
* Your users want to watch the Stream in poor network conditions

Let's show how to broadcast your call to HLS:

```kotlin
call.startBroadcast()
val hlsUrl = call.state.egress.value?.hls
Log.i("Tutorial", "HLS url = $hlsUrl")
```

You can view the HLS video feed using any HLS capable video player.

### 7 - Advanced Features

This tutorial covered broadcasting and watching a livestream.
It also went into more details about HLS & RTMP-in.

There are several advanced features that can improve the livestreaming experience:

TODO: docs links

* ** [Co-hosts](../03-guides/02-joining-creating-calls.mdx) ** You can add members to your livestream with elevated permissions. So you can have co-hosts, moderators etc.
* ** [Custom events](../03-guides/08-reactions-and-custom-events.mdx) ** You can use custom events on the call to share any additional data. Think about showing the score for a game, or any other realtime use case.
* ** [Reactions & Chat](../03-guides/08-reactions-and-custom-events.mdx) ** Users can react to the livestream, and you can add chat. This makes for a more engaging experience.
* ** [Notifications](../06-advanced/01-ringing.mdx) ** You can notify users via push notifications when the livestream starts
* ** [Recording](../06-advanced/06-recording.mdx) ** The call recording functionality allows you to record the call with various options and layouts

### Recap

It was fun to see just how quickly you can build in-app low latency livestreaming.
Please do let us know if you ran into any issues.
Our team is also happy to review your UI designs and offer recommendations on how to achieve it with Stream.

To recap what we've learned:

* WebRTC is optimal for latency, HLS is slower but buffers better for users with poor connections
* You setup a call: (val call = client.call("livestream", callId))
* The call type "livestream" controls which features are enabled and how permissions are setup
* The livestream by default enables "backstage" mode. This allows you and your co-hosts to setup your mic and camera before allowing people in
* When you join a call, realtime communication is setup for audio & video: (call.join())
* Stateflow objects in call.state and call.state.participants make it easy to build your own UI
* For a livestream the most important one is call.state.???

Calls run on Stream's global edge network of video servers.
Being closer to your users improves the latency and reliability of calls.
The SDKs enable you to build livestreaming, audio rooms and video calling in days.

We hope you've enjoyed this tutorial and please do feel free to reach out if you have any suggestions or questions.